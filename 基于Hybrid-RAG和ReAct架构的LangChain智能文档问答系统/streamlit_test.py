
# å¯¼å…¥æ„å»ºé“¾çš„ç›¸å…³åº“

from langchain.chat_models import init_chat_model
from langchain_openai.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.agents import create_react_agent,AgentExecutor
# å¯¼å…¥RAGç³»ç»Ÿæ‰€éœ€çš„åº“
import os
from langchain.tools import tool
from PyPDF2 import PdfReader    #pdfè¯»å–
from langchain.text_splitter import RecursiveCharacterTextSplitter  #æ–‡æ¡£åˆ‡åˆ†
from langchain_community.embeddings import DashScopeEmbeddings  #è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°çš„embeddingæ¨¡å‹
from langchain_community.vectorstores import FAISS  #ä½¿ç”¨FAISSå‘é‡æ•°æ®åº“å­˜å‚¨åˆ‡åˆ†å¥½çš„æ–‡æœ¬å‘é‡
# å¯¼å…¥è®°å¿†åº“
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from dotenv import load_dotenv
load_dotenv()

# 1.åˆ›å»ºpdfè¯»å–å’Œåˆ†å—å‡½æ•°
## 1.1 pdfè¯»å–å‡½æ•°
def pdf_read(pdf_doc):
    text=""     #å­˜å‚¨æ‰€æœ‰æå–çš„æ–‡æœ¬å†…å®¹
    for pdf in pdf_doc:     #å¾ªç¯è¯»å–æ¯ä¸€ä¸ªpdfæ–‡ä»¶
        print(pdf)
        pdf_reader=PdfReader(pdf)   #PdfReaderè¯»å–æ–‡ä»¶åï¼Œä¼šè¿”å›é¡µé¢ä¿¡æ¯
        for page in pdf_reader.pages:   #ä¸€é¡µä¸€é¡µçš„å¤„ç†
            page_text=page.extract_text()   #æå–æ¯ä¸€é¡µçš„æ–‡ä»¶å†…å®¹
            text=text+page_text             #ç»„æˆtext
    return text

## 1.2 textåˆ‡å—å‡½æ•°
def get_chunks(text):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200) #åˆ›å»ºä¸€ä¸ªæ–‡æœ¬åˆ†å—çš„å®ä¾‹å¯¹è±¡
    chunks=text_splitter.split_text(text)
    return chunks

# 2.åˆå§‹åŒ–å‘é‡æ¨¡å‹
embeddings=DashScopeEmbeddings(
    model='text-embedding-v1',
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")
)
# 3.ä½¿ç”¨embeddingæ¨¡å‹å‘é‡åŒ–åˆ‡å—åçš„æ–‡æœ¬å¹¶å­˜å…¥FAISSå‘é‡æ•°æ®åº“ä¸­â€”â€”åˆ›å»ºå‘é‡è½¬æ¢å‡½æ•°
def vector_store(chunks):
    vector=FAISS.from_texts(chunks,embedding=embeddings)
    vector.save_local("faiss_db1")
    # # ä¿å­˜æ–‡æœ¬å—ç”¨äºBM25å…³é”®è¯æ£€ç´¢
    # global text_chunks_for_bm25
    # text_chunks_for_bm25 = chunks

# 4.æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
def check_database_exists():
    """æ£€æŸ¥FAISSæ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
    return os.path.exists("faiss_db1") and os.path.exists("faiss_db1/index.faiss")


# 5.ä½¿ç”¨get_chunkså‡½æ•°è¿”å›çš„chunksæ„å»ºWhooshå€’æ’ç´¢å¼•å¹¶å­˜å‚¨ï¼Œä¾¿äºåç»­è¿›è¡Œå…³é”®è¯æœç´¢
from whoosh.fields import Schema, TEXT
from whoosh.index import create_in
import os
from jieba.analyse import ChineseAnalyzer
from langdetect import detect

def whoosh_index_store(chunks):
    text=" ".join(chunks)
    lang=detect(text)
    if lang=="zh":
        an=ChineseAnalyzer()
        schema=Schema(content=TEXT(stored=True,analyzer=an))
    else:
        schema=Schema(content=TEXT(stored=True))
    if not os.path.exists("whoosh_index"):
        os.mkdir("whoosh_index")
    ix=create_in("whoosh_index",schema)
    writer=ix.writer()
    for chunk in chunks:
        writer.add_document(content=chunk)
    writer.commit()


# 6.åˆ›å»ºä¸€ä¸ªragæ£€ç´¢å‡½æ•°ï¼Œç”¨äºè¿”å›æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢åˆ°çš„æ–‡æœ¬å†…å®¹
@tool
# def rag_search(user_inputs,path):
def rag_search(user_inputs):
    """æ ¹æ®ç”¨æˆ·é—®é¢˜ä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³çš„å†…å®¹"""
    print(f"æ­£åœ¨æŸ¥æ‰¾å·¥å…·")
    path="faiss_db"
    new_db=FAISS.load_local(path,embeddings,allow_dangerous_deserialization=True)
    docs=new_db.similarity_search(user_inputs,k=3)
    return "\n\n".join([doc.page_content for doc in docs])


# 6.åˆ›å»ºä¸€ä¸ªhybrid_searchå‡½æ•°â€”â€”â€”â€”å‘é‡åŠ å…³é”®è¯æ£€ç´¢
    ## 6.1 å¯¼å…¥å…³é”®è¯æ£€ç´¢çš„åº“
from whoosh.index  import open_dir
from whoosh import scoring
from whoosh.qparser import QueryParser,OrGroup
from transformers import AutoTokenizer,AutoModelForSequenceClassification
import torch
import numpy as np

def normalize(scores):
    '''å¯¹å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢çš„åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¼©æ”¾å½“ç»Ÿä¸€å°ºåº¦æ–¹ä¾¿è¿›è¡ŒåŠ æƒèåˆ'''
    min_score=min(scores.values())
    max_score=max(scores.values())
    return {k:(v-min_score)/(max_score-min_score+1e-8) for k,v in scores.items()}

    ## 6.2 åˆå§‹åŒ–rerankæ¨¡å‹
tokenizer=AutoTokenizer.from_pretrained("BAAI/bge-reranker-base")
reranker=AutoModelForSequenceClassification.from_pretrained("BAAI/bge-reranker-base")
    ## 6.3 æƒé‡è®¾ç½®
bm25_weight=0.4
faiss_weight=0.6
top_k=5

@tool
def hybrid_search(user_inputs):
    """1.å‘é‡æ£€ç´¢â€”â€”æ ¹æ®ç”¨æˆ·é—®é¢˜ä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³çš„å†…å®¹"""
    path="faiss_db1"
    new_db=FAISS.load_local(path,embeddings,allow_dangerous_deserialization=True)
    docs=new_db.similarity_search_with_score(user_inputs,k=10)
    faiss_chunks = {doc.page_content: score for doc,score in docs}
    print(faiss_chunks)
    print("\n")

    """2.å…³é”®è¯æ£€ç´¢â€”â€”æå‰ç”¨æˆ·é—®é¢˜ä¸­çš„å…³é”®è¯ï¼Œæ¯”å¦‚ä¸€äº›ä¸“ä¸šåè¯çš„é—®é¢˜"""
    ix=open_dir("whoosh_index")
    print("\n whoosh_indexå­˜åœ¨")
    with ix.searcher(weighting=scoring.BM25F()) as searcher:
        parser=QueryParser("content",schema=ix.schema,group=OrGroup.factory(0.9))
        myquery=parser.parse(user_inputs)
        bm25_results=searcher.search(myquery,limit=10)
        print(bm25_results)
        bm25_chunks={hit["content"]:hit.score for hit in bm25_results}
        print(bm25_chunks)

    """3. åˆå¹¶å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢çš„ç»“æœå¹¶åŠ æƒï¼Œå¯¹æœ€ç»ˆç»“æœé€‰å–å‰top-kçš„ç»“æœ"""
    all_chunks = {}
    if bm25_chunks:
        bm25_chunks = normalize(bm25_chunks)
    print("1\n")
    if faiss_chunks:
        faiss_chunks = normalize(faiss_chunks)

    for chunk, score in bm25_chunks.items():
        all_chunks[chunk] = all_chunks.get(chunk, 0) + bm25_weight * score
    for chunk, score in faiss_chunks.items():
        all_chunks[chunk] = all_chunks.get(chunk, 0) + faiss_weight * score
    sorted_chunks=sorted(all_chunks.items(),key=lambda x:x[1],reverse=True)
    candidate_chunks=[chunk for chunk,_ in sorted_chunks[:top_k]]

    """4.Rerank"""
    print("æ­£åœ¨æ‰§è¡ŒRerank..")
    rerank_inputs=tokenizer(
        [f"{user_inputs}[SEP]{chunk}" for chunk in candidate_chunks],
        padding=True,
        truncation=True,
        return_tensors="pt"
    )
    with torch.no_grad():
        scores = reranker(**rerank_inputs).logits.squeeze(-1)
    reranked = sorted(zip(candidate_chunks, scores.tolist()), key=lambda x: x[1], reverse=True)

    """5. è¿”å›æœ€ç»ˆç­›é€‰ç»“æœ"""
    final_context = "\n\n".join([chunk for chunk, _ in reranked])
    return final_context
# 7.åˆå§‹åŒ–æ¨¡å‹
llm=ChatOpenAI(
    model="deepseek-chat",
    temperature=0,
    openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1"
)
# 8.åˆ›å»ºå¯¹è¯å†å²
history_store={}
def get_session_history(session_id):
    if session_id not in history_store:
        history_store[session_id]=ChatMessageHistory()
    return history_store[session_id]

## 9.åŠ è½½å·¥å…·
# tools=[rag_search]
tools=[hybrid_search]

## 10. æ„å»ºåŸºäºReActçš„prompt
from langchain_core.prompts import PromptTemplate
template= """
    Answer the following questions as best you can. You have access to the following tools:
    {tools}
    Use the following format:
    Question: the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action
    Observation: the result of the action
    ... (this Thought/Action/Action Input/Observation can repeat 3 times)
    Thought: I now know the final answer, don't need to take a action. give the Final Answer
    Final Answer: the final answer to the original input question
    Begin!
    chat_history:{chat_history}
    Question: {input}
    Thought:{agent_scratchpad}
"""
prompt = PromptTemplate.from_template(template)

# 11. åˆ›å»ºå‰ç«¯å±•ç¤ºç•Œé¢â€”â€”é“¾æ¥å¤šè½®å¯¹è¯
import streamlit as st
import tempfile

def UI():
    ## 1.è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
    st.set_page_config(
        page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    ## 2.è®¾ç½®ç•Œé¢é¡¶éƒ¨çš„æ ‡é¢˜å’Œé—´æ¥
    st.markdown("""
    <div style="text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: #1f77b4; margin: 0;">ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿ</h1>
        <p style="color: #666; margin: 5px 0 0 0;">åŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ</p>
    </div>
    """, unsafe_allow_html=True)
    ## 3.åˆ›å»ºé¡µé¢å¸ƒå±€
    col1,col2=st.columns([1,2])

    ## 4.è®¾ç½®å·¦å³ä¸¤åˆ—ä¸»è¦çš„é¡µé¢å†…å®¹
        ### å·¦ä¾§â€”â€”ä¸Šä¼ æ–‡æ¡£å’Œä½¿ç”¨è¯´æ˜
    with col1:
        #### ä½¿ç”¨è¯´æ˜åŒºåŸŸ
        st.markdown("### âš™ï¸ ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1.åœ¨å·¦ä¾§ä¸Šä¼ PDFæ–‡æ¡£
        2.ç­‰å¾…æ–‡æ¡£å¤„ç†å®Œæˆ
        3.åœ¨å³ä¾§è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯            
        """)

        #### æ–‡æ¡£åŠ è½½åŒºåŸŸ
        st.markdown("### ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
        #1.æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
        uploaded_files=st.file_uploader(
            "ä¸Šä¼ PDFæ–‡æ¡£",
            type=['pdf'],
            accept_multiple_files=True,
            help="æ”¯æŒä¸Šä¼ å¤šä¸ªPDFæ–‡ä»¶"
        )
        #2.å¦‚æœæ–‡æ¡£ä¸Šä¼ å¥½äº†ï¼Œåˆ™å¼€å§‹â€œå¤„ç†æ–‡æ¡£â€ï¼Œç‚¹å‡»å¤„ç†æ–‡æ¡£çš„æŒ‰é’®ï¼Œåˆ™ä¼šå‡ºç°ä¸€ä¸ªçŠ¶æ€â€œå‘é‡æ•°æ®åº“å·²å°±ç»ªï¼Œåä¹‹åˆ™æç¤ºè¯·å…ˆä¸Šä¼ æ–‡æ¡£â€
        if uploaded_files:
            if st.button("ğŸš€ å¤„ç†æ–‡æ¡£",use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£...."):
                    try:
                        # ä¿å­˜ä¸Šä¼ çš„æ‰€æœ‰æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ï¼Œä¾¿äºåç»­è¯»å–
                        temp_files=[]   #é‡Œé¢å­˜å‚¨çš„æ˜¯ä¸Šä¼ çš„æ‰€æœ‰pdfçš„æœ¬åœ°è·¯å¾„

                        for file in uploaded_files:
                            temp_file=tempfile.NamedTemporaryFile(delete=False,suffix='.pdf')
                            temp_file.write(file.getvalue())
                            temp_files.append(temp_file.name)
                        
                        # å¤„ç†æ–‡æ¡£
                        print(f"åŠ è½½çš„æ–‡ä»¶æœ‰ï¼š{temp_files}")
                        text = pdf_read(temp_files)
                        print("æ–‡æ¡£å†…å®¹è¯»å–æˆåŠŸ")
                        text_chunks = get_chunks(text)
                        print("åˆ†å—æˆåŠŸ")
                        vector_store(text_chunks)
                        print("å‘é‡åº“å‡†å¤‡æˆåŠŸ")
                        whoosh_index_store(text_chunks)
                        print("ç´¢å¼•åº“å‡†å¤‡æˆåŠŸ")

                        #åˆ¤æ–­æ˜¯å¦å¤„ç†æˆåŠŸï¼ŒæˆåŠŸï¼Œè´¼æ˜¾ç¤ºâ€œæ•°æ®åº“å·²å°±ç»ªâ€ï¼Œåä¹‹â€œè¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–æ•°æ®è¯»å–å¤±è´¥â€
                        if check_database_exists():
                            st.success("âœ… æ•°æ®åº“å·²å°±ç»ª")
                            st.success(f"âœ… æˆåŠŸå¤„ç† {len(uploaded_files)} ä¸ªæ–‡æ¡£ï¼Œå…± {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
                        else:
                            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£æˆ–æ•°æ®è¯»å–å¤±è´¥")
                        
                    except Exception as e:
                        st.error(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        #### æ–‡æ¡£åŠ è½½åŒºåŸŸ
        st.markdown("### ğŸ”„ å¯¹è¯é‡ç½®")
        if st.button("ğŸ”„ æ¸…ç©ºèŠå¤©è®°å½•"):
            st.session_state.messages=[]
            history_store["user-123"]=ChatMessageHistory()
            st.success("å¯¹è¯å·²é‡ç½®ï¼Œå¯ä»¥é‡æ–°å¼€å§‹")
            
     ### å³ä¾§â€”â€”å¯¹è¯åŒº
    with col2:
        #### åˆå§‹åŒ–å¯¹è¯å†å²
        if "messages" not in st.session_state:
            st.session_state.messages=[]

        if "conversation_chain" not in st.session_state:
            agent = create_react_agent(llm, tools, prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=10,
                early_stopping_method="force",
            )
            st.session_state.conversation_chain = RunnableWithMessageHistory(
                agent_executor,
                get_session_history=get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history"
            )
        #### è®¾ç½®å¯¹è¯åŒºçš„å¤§å°
        chat_container = st.container(height=600)
            #### å°†å¯¹è¯å†…å®¹åŠ è½½åˆ°å¯¹è¯åŒº
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
        #### ç”¨æˆ·é—®é¢˜è¾“å…¥
        user_inputs=st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",key="user_inputs")  #ç”¨æˆ·è¾“å…¥
        if user_inputs:
            st.session_state.messages.append({"role":"user","content":user_inputs})
            #åœ¨å¯¹è¯åŒºæ˜¾ç¤ºç”¨æˆ·é—®é¢˜
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(user_inputs)

            with st.spinner("æ­£åœ¨æ€è€ƒ...."):
                try:

                    response=st.session_state.conversation_chain.invoke({'input':user_inputs},config={"configurable": {"session_id": "user-123"}})
                    print(response.get("output","æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"))
                    #å°†æ¨¡å‹ç”Ÿæˆçš„å†…å®¹æ·»åŠ åˆ°å¯¹è¯å†å²çŠ¶æ€å¹¶æ˜¾ç¤º
                    response_text=response.get("output","æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜")
                    st.session_state.messages.append({"role":"assistant","content":response_text})
                    with chat_container:
                        with st.chat_message("assistant"):
                            st.markdown(response_text)


                except Exception as e:
                    error_msg=f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    with chat_container:
                        with st.chat_message("assistant"):
                            st.markdown(error_msg)

if __name__=="__main__":
    UI()


    

